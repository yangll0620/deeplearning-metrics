## Brief Description

Cohen's kappa coefficient (κ) is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance. (From Wikepedia)

## Definition
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\kappa=\frac{p_0-p_e}{1-p_e}" title="kappa2classes" />


h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x

For categories <em>k</em>, number of items <em>N</em> and <em>n</em><sub>ki</sub> the number of times rater <em>i</em> predicted category <em>k</em>:

<img src="https://latex.codecogs.com/svg.latex?\Large&space;\p_e=\frac{1}{N^2}\sum_{k}{n_{k1}n_{k2}}" title="peCalculation" />

