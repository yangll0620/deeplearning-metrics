## Brief Description

Cohen's kappa coefficient (κ) is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance. (From Wikepedia)

## Definition
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\kappa=\frac{p_0-p_e}{1-p_e}" title="kappa2classes" />


h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x

For categories <em>k</em>, number of items <em>N</em> and {\displaystyle n_{ki}} {\displaystyle n_{ki}} the number of times rater i predicted category k:
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\p_e=\sum_{}" title="peCalculation" />

